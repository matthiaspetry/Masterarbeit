{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":7085371,"sourceType":"datasetVersion","datasetId":4082304},{"sourceId":7091487,"sourceType":"datasetVersion","datasetId":4086575},{"sourceId":7101392,"sourceType":"datasetVersion","datasetId":4088244},{"sourceId":7139549,"sourceType":"datasetVersion","datasetId":4120458},{"sourceId":7149574,"sourceType":"datasetVersion","datasetId":4082281},{"sourceId":7224870,"sourceType":"datasetVersion","datasetId":4182332},{"sourceId":7225002,"sourceType":"datasetVersion","datasetId":4182416},{"sourceId":7225245,"sourceType":"datasetVersion","datasetId":4182602},{"sourceId":7233479,"sourceType":"datasetVersion","datasetId":4188605},{"sourceId":7240482,"sourceType":"datasetVersion","datasetId":4193572},{"sourceId":7250788,"sourceType":"datasetVersion","datasetId":4200977},{"sourceId":7292907,"sourceType":"datasetVersion","datasetId":4229848},{"sourceId":7299057,"sourceType":"datasetVersion","datasetId":4234197},{"sourceId":7324623,"sourceType":"datasetVersion","datasetId":4251172},{"sourceId":7364451,"sourceType":"datasetVersion","datasetId":4278207},{"sourceId":7365885,"sourceType":"datasetVersion","datasetId":4279158},{"sourceId":7379699,"sourceType":"datasetVersion","datasetId":4288592},{"sourceId":7379714,"sourceType":"datasetVersion","datasetId":4288606},{"sourceId":7391050,"sourceType":"datasetVersion","datasetId":4296584},{"sourceId":7422082,"sourceType":"datasetVersion","datasetId":4318345},{"sourceId":7547984,"sourceType":"datasetVersion","datasetId":4395965},{"sourceId":7550821,"sourceType":"datasetVersion","datasetId":4397779},{"sourceId":7563615,"sourceType":"datasetVersion","datasetId":4404156},{"sourceId":7573037,"sourceType":"datasetVersion","datasetId":4408764},{"sourceId":7590801,"sourceType":"datasetVersion","datasetId":4418630},{"sourceId":7657894,"sourceType":"datasetVersion","datasetId":4464963},{"sourceId":7665616,"sourceType":"datasetVersion","datasetId":4470450},{"sourceId":7665641,"sourceType":"datasetVersion","datasetId":4470472},{"sourceId":7665662,"sourceType":"datasetVersion","datasetId":4470490},{"sourceId":7671834,"sourceType":"datasetVersion","datasetId":4474826},{"sourceId":7671870,"sourceType":"datasetVersion","datasetId":4474851},{"sourceId":7681090,"sourceType":"datasetVersion","datasetId":4481336},{"sourceId":7688577,"sourceType":"datasetVersion","datasetId":4486792},{"sourceId":7715660,"sourceType":"datasetVersion","datasetId":4506103},{"sourceId":7716175,"sourceType":"datasetVersion","datasetId":4506429},{"sourceId":7724317,"sourceType":"datasetVersion","datasetId":4512494},{"sourceId":7728252,"sourceType":"datasetVersion","datasetId":4515444},{"sourceId":7731468,"sourceType":"datasetVersion","datasetId":4517831},{"sourceId":7731502,"sourceType":"datasetVersion","datasetId":4517857},{"sourceId":7731543,"sourceType":"datasetVersion","datasetId":4517888},{"sourceId":7838138,"sourceType":"datasetVersion","datasetId":4594650},{"sourceId":7853958,"sourceType":"datasetVersion","datasetId":4606330},{"sourceId":7879820,"sourceType":"datasetVersion","datasetId":4624763}],"dockerImageVersionId":30665,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch \nimport torchvision\nimport numpy as np\nimport random\ndef set_seed(seed):\n    torch.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n    np.random.seed(seed)\n    random.seed(seed)\n\n# Set the seed value (you can choose any integer value)\nseed = 420\nset_seed(seed)","metadata":{"execution":{"iopub.status.busy":"2024-05-07T10:56:46.243877Z","iopub.execute_input":"2024-05-07T10:56:46.244122Z","iopub.status.idle":"2024-05-07T10:56:52.775507Z","shell.execute_reply.started":"2024-05-07T10:56:46.244099Z","shell.execute_reply":"2024-05-07T10:56:52.774622Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"import torch\nimport numpy as np\nfrom PIL import Image\nfrom torch.utils.data import Dataset\nfrom torchvision import transforms\nimport os\n\nclass Image2JointPosition(Dataset):\n    def __init__(self, root_dir, transform=None):\n        self.root_dir = root_dir\n        self.transform = transform\n        self.data = self.read_data()\n        self.mean, self.std = self.compute_mean_std()\n        self.classes = self.get_unique_classes()\n\n    def get_unique_classes(self):\n        # Extract class labels from data and find unique classes\n        classes = set(int(full_data.split(',')[6]) for full_data in self.data)\n        return sorted(list(classes))\n\n    def one_hot_encode(self, class_idx):\n        # Convert a class index to a one-hot encoded tensor\n        one_hot = torch.zeros(8)\n        one_hot[class_idx] = 1\n        return one_hot\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        img_name = os.path.join(self.root_dir, f'image{idx}.jpg')\n\n        if not os.path.exists(img_name):\n            print(f\"Warning: Image file {img_name} not found.\")\n            return None\n\n        image = Image.open(img_name)\n        if self.transform:\n            image = self.transform(image)\n\n        full_data = self.data[idx]\n        full_data = [float(value) for value in full_data.split(',')]\n\n        data = full_data[:6]\n       \n        \n        class_idx = int(full_data[6])\n  \n        vector = full_data[-4:]\n        \n        # One-hot encode the class\n        class_one_hot = self.one_hot_encode(class_idx)\n\n        # Standard Scaling\n        data = [(val - self.mean) / self.std for val in data]\n        center_x, center_y, width, height = vector\n\n        # Convert xywhn to xyxyn format\n        x1 = center_x - (width / 2)\n        y1 = center_y - (height / 2)\n        x2 = center_x + (width / 2)\n        y2 = center_y + (height / 2)\n\n        # Create the bounding box in xyxyn format\n        bbox_xyxyn = torch.tensor([x1, y1, x2, y2], dtype=torch.float32)\n\n        target = torch.tensor(data, dtype=torch.float32)\n        bbox = torch.tensor(vector, dtype=torch.float32)\n        \n        boxs = torch.cat((bbox, bbox_xyxyn), 0)\n\n        return image, boxs, target\n\n    def read_data(self):\n        #data_file = os.path.join(self.root_dir, 'new_positions.txt')\n        with open(\"/kaggle/input/newyolo/newdataset/positions2n.txt\", 'r') as f:\n            lines = f.readlines()\n\n        # Extract data after the colon and remove newline characters\n        data = [line.split(': ')[1].strip() for line in lines]\n\n        return data\n\n    def compute_mean_std(self):\n        # Extract all data points\n        all_data = [float(value) for line in self.data for value in line.split(',')[:6]]\n        mean = np.mean(all_data)\n        std = np.std(all_data)\n        return mean, std\n\n\nfrom torchvision import transforms\nfrom torchvision.transforms import GaussianBlur\nimport random\n\n\n\n# Updated transform pipeline\ntransform = transforms.Compose([\n    transforms.Resize((256, 256)),\n    #transforms.RandomAffine(degrees=5, translate=(0.05, 0.05), scale=(0.98, 1.02)),  # Rotation, Translation, Scaling\n    transforms.RandomRotation(degrees=20),\n    transforms.RandomPerspective(distortion_scale=0.05, p=0.4),\n    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2),\n    GaussianBlur(kernel_size=(5, 9), sigma=(0.1, 5)),  # Blurring\n    transforms.RandomGrayscale(p=0.2),\n    transforms.RandomVerticalFlip(p=0.3),\n    transforms.RandomPosterize(bits=2,p=0.2),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n])\n\ndataset = Image2JointPosition(root_dir='/kaggle/input/newyolo/newdataset', transform=transform)\n\nmean, std = dataset.compute_mean_std()\nprint(mean)\nprint(std)\n# Accessing the first sample in the dataset\nimage, bbox, target = dataset[0]\nprint(\"Image Size:\", image.shape) \nprint(\"Corresponding bbox:\", bbox)\nprint(\"Corresponding target:\", target)\n\ndef reverse_standard_scaling(mean, std, scaled_data):\n        original_data = [(val * std) + mean for val in scaled_data]\n        return original_data\n    \nprint(reverse_standard_scaling(mean,std,target))\nlen(dataset)","metadata":{"execution":{"iopub.status.busy":"2024-05-07T10:56:52.777187Z","iopub.execute_input":"2024-05-07T10:56:52.777590Z","iopub.status.idle":"2024-05-07T10:56:53.128461Z","shell.execute_reply.started":"2024-05-07T10:56:52.777564Z","shell.execute_reply":"2024-05-07T10:56:53.127544Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"-0.02049437658091184\n1.4781722524455945\nImage Size: torch.Size([3, 256, 256])\nCorresponding bbox: tensor([0.5335, 0.9014, 0.0283, 0.0727, 0.5194, 0.8650, 0.5477, 0.9378])\nCorresponding target: tensor([ 0.8928, -0.7075,  0.6708, -0.9759, -1.0289,  0.8653])\n[tensor(1.2992), tensor(-1.0663), tensor(0.9710), tensor(-1.4630), tensor(-1.5414), tensor(1.2586)]\n","output_type":"stream"},{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"5403"},"metadata":{}}]},{"cell_type":"code","source":"from torch.utils.data import random_split, DataLoader\nfrom torch.utils.data.dataloader import default_collate\n# Determine the size of the training and testing sets\ntrain_size = int(0.8 * len(dataset))  # 80% for training\ntest_size = len(dataset) - train_size  # 20% for testing\n\n# Split the dataset\ntrain_set, test_set = random_split(dataset, [train_size, test_size])\n\n\n# Create DataLoader for training and testing sets with custom collate function\ntrain_loader = DataLoader(train_set, batch_size=32, shuffle=True)\ntest_loader = DataLoader(test_set, batch_size=32, shuffle=True)","metadata":{"execution":{"iopub.status.busy":"2024-05-07T10:56:53.129644Z","iopub.execute_input":"2024-05-07T10:56:53.129931Z","iopub.status.idle":"2024-05-07T10:56:53.137101Z","shell.execute_reply.started":"2024-05-07T10:56:53.129906Z","shell.execute_reply":"2024-05-07T10:56:53.136057Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport timm\n\nclass LayerNormFastViT6DPosition(nn.Module):\n    def __init__(self, dropout_rate=0.1, vector_input_size=8, intermediate_size=128, hidden_layer_size=64):\n        super(LayerNormFastViT6DPosition, self).__init__()\n\n        # Load FastViT model pre-trained on ImageNet resnet18.a1_in1k\n        self.fastvit = timm.create_model('fastvit_t8.apple_dist_in1k', pretrained=True) \n        #self.fastvit = timm.create_model('tf_efficientnet_b1.in1k', pretrained=True) \n        #self.fastvit = timm.create_model('efficientvit_m0.r224_in1k', pretrained=True)\n        in_features = self.fastvit.get_classifier().in_features\n        self.fastvit.reset_classifier(num_classes=0)  # Remove the classifier\n\n        # Model for processing 4D vector input with LayerNorm\n        self.vector_model = nn.Sequential(\n            nn.Linear(vector_input_size, intermediate_size),\n            nn.ReLU(),\n            nn.LayerNorm(intermediate_size),\n            nn.Linear(intermediate_size, in_features),\n            nn.ReLU(),\n            nn.LayerNorm(in_features),\n            nn.Dropout(dropout_rate)\n        )\n\n        # Enhanced combined output layer with LayerNorm\n        self.combined_output_layer = nn.Sequential(\n            nn.Linear(in_features * 2, hidden_layer_size),\n            nn.ReLU(),\n            nn.LayerNorm(hidden_layer_size),\n            nn.Dropout(dropout_rate),\n            nn.Linear(hidden_layer_size, hidden_layer_size),\n            nn.ReLU(),\n            nn.LayerNorm(hidden_layer_size),\n            nn.Linear(hidden_layer_size, 6)\n        )\n\n    def forward(self, x, vector):\n        # Extract features using FastViT\n        fastvit_features = self.fastvit(x)\n\n        # Process the 4D vector through the vector model\n        vector_features = self.vector_model(vector)\n\n        # Concatenate FastViT and vector features\n        concatenated_features = torch.cat((fastvit_features, vector_features), dim=1)\n\n        # Final output layer for regression\n        final_output = self.combined_output_layer(concatenated_features)\n\n        return final_output\n","metadata":{"execution":{"iopub.status.busy":"2024-05-07T10:56:53.139461Z","iopub.execute_input":"2024-05-07T10:56:53.139792Z","iopub.status.idle":"2024-05-07T10:56:54.488546Z","shell.execute_reply.started":"2024-05-07T10:56:53.139764Z","shell.execute_reply":"2024-05-07T10:56:54.487601Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"### import torch\nimport torch.optim as optim\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom sklearn.metrics import mean_absolute_error, r2_score, mean_squared_error, mean_absolute_percentage_error\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\nfrom torch.utils.tensorboard import SummaryWriter\nfrom torch.nn.utils import clip_grad_norm_\nfrom torch.optim.lr_scheduler import CyclicLR\nimport wandb\n\n# create a nn class (just-for-fun choice :-) \nclass RMSELoss(nn.Module):\n    def __init__(self, eps=1e-6):\n        super().__init__()\n        self.mse = nn.MSELoss()\n        self.eps = eps\n        \n    def forward(self,yhat,y):\n        loss = torch.sqrt(self.mse(yhat,y) + self.eps)\n        return loss\n\n\n# Initialize the model, optimizer, scheduler, and loss function\nmodel = LayerNormFastViT6DPosition()\n#optimizer = optim.Adam(model.parameters(), lr=0.00025, weight_decay=1e-5)\n#scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.7, patience=5, verbose=True)\n\n#optimizer = optim.Adam(model.parameters(), lr=0.0005, weight_decay=1e-5)\noptimizer = optim.AdamW(model.parameters(), lr=0.0003, weight_decay=1e-5)\n\n# Adjust learning rate scheduler\nscheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.6, patience=5, verbose=True)\n#scheduler = CyclicLR(optimizer, base_lr=0.0000001, max_lr=0.001, \n                     #step_size_up=5, step_size_down=20, mode='triangular',\n                     #cycle_momentum=False) \n\ncriterion = nn.MSELoss()\n\n# Set the device to GPU if available\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\n\nnum_epochs = 400\nbest_loss = float('inf')\npatience = 40\ncounter = 0\n\ntrain_losses, val_losses= [], []\ntrain_maes, test_maes, train_r2s, test_r2s, train_mapes, test_mapes = [], [], [], [], [], []\n\n\n# Initialize WandB\nwandb.init(project=\"Joint_Position2\", name=\"Training149T8\")\nwandb.watch(model)\n\n# Training loop\nfor epoch in tqdm(range(num_epochs), desc='Training', unit='epoch'):\n    model.train()\n    total_loss = 0.0\n    train_preds, train_targets_list = [], []\n    for batch_idx, (inputs, bbox, targets) in enumerate(train_loader):\n        inputs, bbox, targets = inputs.to(device), bbox.to(device), targets.to(device)\n\n        # Define the closure that computes the loss\n        optimizer.zero_grad()\n        outputs = model(inputs, bbox)  # Compute outputs\n        loss = criterion(outputs, targets)\n        loss.backward()\n        \n        optimizer.step()  # Use the closure in the step method\n        total_loss += loss.item() if loss is not None else 0\n        train_preds.extend(outputs.detach().cpu().numpy())\n        train_targets_list.extend(targets.detach().cpu().numpy())\n        \n\n    avg_train_loss = total_loss / len(train_loader)\n    \n    train_preds = np.array(train_preds)\n    train_targets = np.array(train_targets_list)\n    train_mae = mean_absolute_error(train_targets, train_preds)\n    train_r2 = r2_score(train_targets, train_preds)\n    train_mape = mean_absolute_percentage_error(train_targets, train_preds)\n\n    # Validation step\n    model.eval()\n    val_loss = 0.0\n    val_preds = []\n    val_targets = []\n    with torch.no_grad():\n        for inputs, bbox, targets in test_loader:\n\n            inputs, bbox, targets = inputs.to(device), bbox.to(device), targets.to(device)\n            outputs = model(inputs, bbox)\n            val_loss += criterion(outputs, targets).item()\n            val_preds.extend(outputs.cpu().numpy())\n            val_targets.extend(targets.cpu().numpy())\n\n    avg_val_loss = val_loss / len(test_loader)\n    scheduler.step(avg_val_loss)\n    #scheduler.step()\n\n    # Calculate additional metrics\n    val_preds = np.array(val_preds)\n    val_targets = np.array(val_targets)\n    mae = mean_absolute_error(val_targets, val_preds)\n    #mse = mean_squared_error(val_targets, val_preds)\n    r2 = r2_score(val_targets, val_preds)\n    mape = mean_absolute_percentage_error(val_targets, val_preds)\n    \n    \n    train_losses.append(avg_train_loss)\n    val_losses.append(avg_val_loss)\n    train_maes.append(train_mae)\n    train_r2s.append(train_r2)\n    train_mapes.append(train_mape)\n    test_maes.append(mae)  # mae calculated for validation data as before\n    test_r2s.append(r2)\n    test_mapes.append(mape)\n\n    # Logging with WandB\n    print(f\"Epoch {epoch + 1}/{num_epochs}, Train Loss: {avg_train_loss:.6f}, Val Loss: {avg_val_loss:.6f}, Train Mae: {train_mae:.4f},  Test Mae: {mae:.4f}, R2: {r2:.4f}, MAPE: {mape:.4f}\")\n    wandb.log({\"Train Loss\": avg_train_loss, \"Val Loss\": avg_val_loss, \"MAE\": mae, \"R2\": r2, \"MAPE\": mape})\n\n    # Early stopping and model saving\n    if avg_val_loss < best_loss:\n        best_model_path = f'T8_{epoch+1}_149.pth'  # Define the best model path here\n        best_loss = avg_val_loss\n        torch.save(model.state_dict(), best_model_path)\n        wandb.save(best_model_path)  # Corrected to use the defined best_model_path variable\n        counter = 0\n    else:\n        counter += 1\n        if counter >= patience:\n            print(\"Early stopping\")\n            break\n    if epoch % 10 == 0:\n        torch.save(model.state_dict(), f'model_{epoch+1}.pth')\n\n#b1a31dce498507eb26dd9b8432d6d97d616d237c\n\nplt.figure(figsize=(10, 7))\nplt.plot(train_losses, label='Training Loss')\nplt.plot(val_losses, label='Validation Loss')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.title('Training and Validation Loss')\nplt.legend()\nplt.show()\n\nplt.figure(figsize=(10, 7))\nplt.plot(train_maes, label='Training MAE')\nplt.plot(test_maes, label='Validation MAE')\nplt.xlabel('Epoch')\nplt.ylabel('Mean Absolute Error')\nplt.title('Training and Validation Mean Absolute Error (MAE) over Epochs')\nplt.legend()\nplt.show()\n\n\n\nplt.figure(figsize=(10, 7))\nplt.plot(train_r2s, label='Training R2')\nplt.plot(test_r2s, label='Validation R2')\nplt.xlabel('Epoch')\nplt.ylabel('R2-Score')\nplt.title('Training and Validation R2-Score over Epochs')\nplt.legend()\nplt.show()\n\nplt.figure(figsize=(10, 7))\nplt.plot(train_mapes, label='Training MAPE')\nplt.plot(test_mapes, label='Validation MAPE')\nplt.xlabel('Epoch')\nplt.ylabel('Mean Absolute Percentage Error')\nplt.title('Training and Validation Mean Absolute Percentage Error (MAPE) over Epochs')\nplt.legend()\nplt.show()\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Define a custom color palette\npalette = sns.color_palette()\n\n\n\n# Setting Seaborn style for all plots\nsns.set_style(\"whitegrid\")\nsns.set_context(\"talk\")\n\n# Training and Validation Loss Plot\nplt.figure(figsize=(10, 7))\nplt.plot(train_losses, label='Training Loss', color=palette[0], linewidth=2.5)\nplt.plot(val_losses, label='Validation Loss', color=palette[3], linewidth=2.5, linestyle='--')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.title('Training and Validation Loss')\nplt.legend()\nplt.savefig('training_validation_lossNewBBox.png')\nplt.show()\n\n# Mean Absolute Error (MAE) over Epochs Plot\nplt.figure(figsize=(10, 7))\nplt.plot(train_maes, label='Training MAE', color=palette[0], linewidth=2.5)\nplt.plot(test_maes, label='Validation MAE', color=palette[3], linewidth=2.5,linestyle='--')\nplt.xlabel('Epoch')\nplt.ylabel('Mean Absolute Error')\nplt.title('Mean Absolute Error (MAE) over Epochs')\nplt.legend()\nplt.savefig('test_mae_lossNewBBox.png')\nplt.show()\n\n# R2 Score over Epochs Plot\nplt.figure(figsize=(10, 7))\nplt.plot(train_r2s, label='Training R2 Score', color=palette[0], linewidth=2.5)\nplt.plot(test_r2s, label='Validation R2 Score', color=palette[3], linewidth=2.5,linestyle='--')\nplt.xlabel('Epoch')\nplt.ylabel('R2 Score')\nplt.title('R2 Score over Epochs')\nplt.legend()\nplt.savefig('test_r2_scoreNewBBox.png')\nplt.show()\n\n# Mean Absolute Percentage Error (MAPE) over Epochs Plot\nplt.figure(figsize=(10, 7))\nplt.plot(train_mapes, label='Training MAPE', color=palette[0], linewidth=2.5)\nplt.plot(test_mapes, label='Validation MAPE', color=palette[3], linewidth=2.5,linestyle='--')\nplt.xlabel('Epoch')\nplt.ylabel('Mean Absolute Percentage Error')\nplt.title('Mean Absolute Percentage Error (MAPE) over Epochs')\nplt.legend()\nplt.savefig('test_mapeNewBBox.png')\nplt.show()\n\nwandb.save('training_validation_lossNewBBox.png')\nwandb.save('test_mae_lossNewBBox.png')\nwandb.save('test_r2_scoreNewBBox.png')\nwandb.save('test_mapeNewBBox.png')\n\n\n","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2024-05-07T10:56:54.490001Z","iopub.execute_input":"2024-05-07T10:56:54.490284Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stderr","text":"2024-05-07 10:56:57.104983: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-05-07 10:56:57.105108: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-05-07 10:56:57.229165: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/16.3M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"030e4c1cd92a4bb6a31f1139520829fd"}},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"  ········································\n"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011112502533332088, max=1.0…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3ea9e94680be40a4a50b155e5f27c021"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"wandb version 0.16.6 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.16.3"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20240507_105714-u7lx4qk0</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/petry-matthias98/Joint_Position2/runs/u7lx4qk0' target=\"_blank\">Training149T8</a></strong> to <a href='https://wandb.ai/petry-matthias98/Joint_Position2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/petry-matthias98/Joint_Position2' target=\"_blank\">https://wandb.ai/petry-matthias98/Joint_Position2</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/petry-matthias98/Joint_Position2/runs/u7lx4qk0' target=\"_blank\">https://wandb.ai/petry-matthias98/Joint_Position2/runs/u7lx4qk0</a>"},"metadata":{}},{"name":"stderr","text":"Training:   0%|          | 0/400 [00:00<?, ?epoch/s]","output_type":"stream"}]},{"cell_type":"code","source":"!zip -r new2training.zip /kaggle/working","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!ls","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"plt.figure(figsize=(10, 7))\nplt.plot(train_losses, label='Training Loss')\nplt.plot(val_losses, label='Validation Loss')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.title('Training and Validation Loss')\nplt.legend()\nplt.show()\n\nplt.figure(figsize=(10, 7))\nplt.plot(train_maes, label='Training MAE')\nplt.plot(test_maes, label='Validation MAE')\nplt.xlabel('Epoch')\nplt.ylabel('Mean Absolute Error')\nplt.title('Training and Validation Mean Absolute Error (MAE) over Epochs')\nplt.legend()\nplt.show()\n\n\n\nplt.figure(figsize=(10, 7))\nplt.plot(train_r2s, label='Training R2')\nplt.plot(test_r2s, label='Validation R2')\nplt.xlabel('Epoch')\nplt.ylabel('R2-Score')\nplt.title('Training and Validation R2-Score over Epochs')\nplt.legend()\nplt.show()\n\nplt.figure(figsize=(10, 7))\nplt.plot(train_mapes, label='Training MAPE')\nplt.plot(test_mapes, label='Validation MAPE')\nplt.xlabel('Epoch')\nplt.ylabel('Mean Absolute Percentage Error')\nplt.title('Training and Validation Mean Absolute Percentage Error (MAPE) over Epochs')\nplt.legend()\nplt.show()\n\n","metadata":{}},{"cell_type":"code","source":"from IPython.display import FileLink\nFileLink(r'new2training.zip')","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}